{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Gym\n",
    "\n",
    "by [Inspired Cognition](https://inspiredco.ai)\n",
    "\n",
    "This is a notebook that demonstrates how to simply play around with prompts for text generation, testing **different models** and **different prompts** and evaluating according to **different criteria**.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"prompt-gym.png\"  width=\"256\" height=\"256\">\n",
    "</p>\n",
    "\n",
    "In the example here we test two different companies' text generation models, [OpenAI's GPT-3](https://openai.com/blog/gpt-3-apps/), and [Cohere's text generation models](https://cohere.ai/generate). Evaluation of the models is done with the [Inspired Cognition Critique](https://docs.inspiredco.ai/critique/) tool for text generation evaluation. We demonstrate the case for text summarization on 100 examples from the [CNN-DailyMail dataset](https://huggingface.co/datasets/cnn_dailymail). But you can swap in whatever models, prompts, metrics, and data that you would like to try on other tasks too!\n",
    "\n",
    "By the end of the exploration, you will have a **comprehensive report** of which prompts and models work well along a number of axes, like the actual table below that was generated from this notebook:\n",
    "\n",
    "| Model | Prompt | UniEval (Consistency) | UniEval (Coherence) | UniEval (Fluency) | UniEval (Relevance) | BartScore (Coverage) | Length Ratio |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| cohere_medium | standard | 0.7466 | 0.4006 | 0.8869 | 0.3438 | -3.4095 | 2.5533 |\n",
    "| cohere_medium | tldr | 0.5006 | 0.2967 | 0.8539 | 0.3312 | -3.1348 | 2.5800 |\n",
    "| cohere_medium | concise | 0.8542 | 0.6115 | 0.9140 | 0.6167 | -3.4220 | 2.4500 |\n",
    "| cohere_medium | complete | 0.8331 | 0.4845 | 0.8825 | 0.5214 | -3.1689 | 2.6767 |\n",
    "| openai_babbage_001 | standard | 0.9409 | 0.9036 | 0.8782 | 0.7975 | -3.4083 | 2.0800 |\n",
    "| openai_babbage_001 | tldr | 0.8728 | 0.9072 | 0.9593 | 0.8145 | -3.5234 | 1.0200 |\n",
    "| openai_babbage_001 | concise | 0.9483 | 0.9365 | 0.8669 | 0.8431 | -3.2528 | 2.2800 |\n",
    "| openai_babbage_001 | complete | 0.9306 | 0.8278 | 0.8634 | 0.6951 | -3.2720 | 2.2633 |\n",
    "| openai_ada_001 | standard | 0.6750 | 0.7270 | 0.8850 | 0.8174 | -3.6719 | 2.0067 |\n",
    "| openai_ada_001 | tldr | 0.7999 | 0.7122 | 0.7973 | 0.6728 | -3.7436 | 1.5300 |\n",
    "| openai_ada_001 | concise | 0.7776 | 0.7439 | 0.8106 | 0.5852 | -3.6096 | 2.3600 |\n",
    "| openai_ada_001 | complete | 0.7732 | 0.5008 | 0.7332 | 0.3283 | -3.5246 | 2.4567 |\n",
    "\n",
    "Some pointers into how to do **further exploration** into the results, such as finding examples where a particular method is doing well or poorly, or where one method is outperforming the other.\n",
    "\n",
    "If you want to discuss more, you can join the [Inspired Cognition Discord](https://discord.com/invite/vJHdpCBqWN) or get in touch through our [contact page](https://inspiredco.ai/contact/), we love talking about applications of generative AI!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we import the necessary libraries and set up our API keys.\n",
    "\n",
    "To install the requirements, run:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "You can get the necessary API keys here:\n",
    "* [OpenAI API Key](https://openai.com/blog/openai-api/)\n",
    "* [Cohere API Key](https://cohere.ai/)\n",
    "* [Inspired Cognition API Key](https://dashboard.inspiredco.ai)\n",
    "\n",
    "Then, create a file called `.env` in the same directory as this notebook, and add the following lines (with ... replaced by your API keys)\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=...\n",
    "COHERE_API_KEY=...\n",
    "IC_API_KEY=...\n",
    "```\n",
    "\n",
    "Finally execute the following cell to set everything up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import cohere\n",
    "import inspiredco.critique\n",
    "import openai\n",
    "import tqdm\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Set up API credentials\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "cohere_api_key = os.environ[\"COHERE_API_KEY\"]\n",
    "co = cohere.Client(cohere_api_key)\n",
    "inspiredco_api_key = os.environ[\"INSPIREDCO_API_KEY\"]\n",
    "critique = inspiredco.critique.Critique(inspiredco_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Models\n",
    "\n",
    "Next, you'll want to decide which models and configurations you'll want to use. These should follow the configuration supported by the provider. You can see more info about the generation APIs supported by each here:\n",
    "\n",
    "* [OpenAI API Doc](https://beta.openai.com/docs/api-reference/completions/create)\n",
    "* [Cohere API Doc](https://cohere.ai/docs/api/)\n",
    "\n",
    "For demonstration purposes, we use the smaller versions of each model (`text-babbage-001` and `text-ada-001` for OpenAI, and `medium` for Cohere), but you can change these to the larger versions (`text-davinci-003` for OpenAI and `xlarge` for Cohere) if you want to try them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which models you want to use\n",
    "models = {\n",
    "    \"cohere_medium\": {\n",
    "        \"provider\": \"cohere\",\n",
    "        \"config\": {\n",
    "            \"model\": \"medium\",\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 100,\n",
    "            \"top_p\": 1,\n",
    "        }\n",
    "    },\n",
    "    \"openai_babbage_001\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"config\": {\n",
    "            \"model\": \"text-babbage-001\",\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 100,\n",
    "            \"top_p\": 1,\n",
    "        }\n",
    "    },\n",
    "    \"openai_ada_001\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"config\": {\n",
    "            \"model\": \"text-ada-001\",\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 100,\n",
    "            \"top_p\": 1,\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Prompts\n",
    "\n",
    "Next, you will want to choose prompts. For the prompts, the input text will be input into the placeholder `[X]`.\n",
    "\n",
    "The examples below are for text summarization, but you can change the prompts to do other tasks as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the prompts you want to use\n",
    "prompts = {\n",
    "    \"standard\": \"Summarize the following text:\\n[X]\\n\\nSummary:\",\n",
    "    \"tldr\": \"[X]\\nTL;DR:\",\n",
    "    \"concise\": \"Write a short and concise summary of the following text:\\n[X]\\n\\nSummary:\",\n",
    "    \"complete\": \"Write a complete summary of the following text:\\n[X]\\n\\nSummary:\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Evaluation Metrics\n",
    "\n",
    "Finally, you'll want to decide which metrics you use to evaluate the quality of the generated text. Critique supports a wide variety of metrics, so you'll want to pick appropriate ones for your task. You can read more about this on the following pages:\n",
    "* [Critique Evaluation Criteria](https://docs.inspiredco.ai/critique/criteria.html)\n",
    "* [Critique Metrics](https://docs.inspiredco.ai/critique/metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"UniEval (Consistency)\": {\n",
    "        \"metric\": \"uni_eval\",\n",
    "        \"config\": {\"task\": \"summarization\", \"evaluation_aspect\": \"consistency\"},\n",
    "    },\n",
    "    \"UniEval (Coherence)\": {\n",
    "        \"metric\": \"uni_eval\",\n",
    "        \"config\": {\"task\": \"summarization\", \"evaluation_aspect\": \"coherence\"},\n",
    "    },\n",
    "    \"UniEval (Fluency)\": {\n",
    "        \"metric\": \"uni_eval\",\n",
    "        \"config\": {\"task\": \"summarization\", \"evaluation_aspect\": \"fluency\"},\n",
    "    },\n",
    "    \"UniEval (Relevance)\": {\n",
    "        \"metric\": \"uni_eval\",\n",
    "        \"config\": {\"task\": \"summarization\", \"evaluation_aspect\": \"relevance\"},\n",
    "    },\n",
    "    \"BartScore (Coverage)\": {\n",
    "        \"metric\": \"bart_score\",\n",
    "        \"config\": {\"model\": \"facebook/bart-large-cnn\", \"variety\": \"reference_target_bidirectional\"},\n",
    "    },\n",
    "    \"Length Ratio\": {\n",
    "        \"metric\": \"length_ratio\",\n",
    "        \"config\": {},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Data\n",
    "\n",
    "Now we'll load our data! Put the inputs in a jsonl file, where the input is in the `source` field and a gold-standard output is in the `reference` field.\n",
    "The source and target examples in this repo are 10 documents and summaries from the CNN-DailyMail dataset, but you can swap in whatever data you want. You'll probably want to use more examples for robust results in practice, but we use a small number here for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input_data.jsonl\", \"r\") as f:\n",
    "    input_data = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Output Text\n",
    "\n",
    "Iterate through the models and prompts and generate the output text. This may take a little while to hit the APIs many times. This will also write out the generated data to `output_data/targets.json` so you can re-run the evaluation step later without having to generate outputs again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = {}\n",
    "for model_name, model_info in models.items():\n",
    "    config, provider = model_info[\"config\"], model_info[\"provider\"]\n",
    "    targets[model_name] = {}\n",
    "    for prompt_name, prompt_template in prompts.items():\n",
    "        my_data = []\n",
    "        for input in tqdm.tqdm(input_data, desc=f\"Generating {model_name=} {prompt_name=}\"):\n",
    "            source = input[\"source\"]\n",
    "            if provider == \"openai\":\n",
    "                response = openai.Completion.create(\n",
    "                    engine=config[\"model\"],\n",
    "                    prompt=prompt_template.replace(\"[X]\", source),\n",
    "                    temperature=config[\"temperature\"],\n",
    "                    max_tokens=config[\"max_tokens\"],\n",
    "                    top_p=config[\"top_p\"],\n",
    "                )\n",
    "                my_data.append(response[\"choices\"][0][\"text\"])\n",
    "            elif provider == \"cohere\":\n",
    "                response = co.generate(  \n",
    "                    model=config[\"model\"],  \n",
    "                    prompt=prompt_template.replace(\"[X]\", source),\n",
    "                    temperature=config[\"temperature\"],  \n",
    "                    max_tokens=config[\"max_tokens\"],\n",
    "                    p=config[\"top_p\"], \n",
    "                )\n",
    "                my_data.append(response.generations[0].text)\n",
    "                time.sleep(10)  # Sleep to avoid rate limiting on developer API\n",
    "            else:\n",
    "                raise ValueError(\"Unknown provider, but you can add your own!\")\n",
    "        targets[model_name][prompt_name] = my_data\n",
    "if not os.path.exists(\"output_data\"):\n",
    "    os.makedirs(\"output_data\")\n",
    "with open(\"output_data/targets.json\", \"w\") as f:\n",
    "    json.dump(targets, f, indent=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Output Text\n",
    "\n",
    "Now we'll evaluate the output text from a number of different perspectives. We'll also save the evaluation results for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critique_data = {}\n",
    "# Dispatch jobs\n",
    "for model_name, model_data in targets.items():\n",
    "    critique_data[model_name] = {}\n",
    "    for prompt_name, target_data in model_data.items():\n",
    "        critique_data[model_name][prompt_name] = {}\n",
    "        print(f\"Submitting evaluation jobs for {model_name=} {prompt_name=}\")\n",
    "        for metric_name, metric_info in metrics.items():\n",
    "            metric, config = metric_info[\"metric\"], metric_info[\"config\"]\n",
    "            dataset = [\n",
    "                {\"source\": input[\"source\"], \"target\": target, \"references\": [input[\"reference\"]]}\n",
    "                for input, target in zip(input_data, target_data)\n",
    "            ]\n",
    "            critique_data[model_name][prompt_name][metric_name] = critique.submit_task(\n",
    "                metric=metric,\n",
    "                config=config,\n",
    "                dataset=dataset,\n",
    "            )\n",
    "\n",
    "# Collect results\n",
    "for model_name, model_data in critique_data.items():\n",
    "    for prompt_name, prompt_data in model_data.items():\n",
    "        print(f\"Retrieving evaluation results for {model_name=} {prompt_name=}\")\n",
    "        for metric_name, task_id in prompt_data.items():\n",
    "            prompt_data[metric_name] = critique.wait_for_result(task_id)\n",
    "with open(\"output_data/evaluations.json\", \"w\") as f:\n",
    "    json.dump(critique_data, f, indent=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Evaluation Table\n",
    "\n",
    "Finally, we'll generate a table of the evaluation results, and save it to `output_data/evaluation_table.md`, as well as displaying it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_data/evaluation_table.md\", \"w\") as f:\n",
    "    def print_both(text):\n",
    "        print(text)\n",
    "        print(text, file=f)\n",
    "    metric_list = list(metrics.keys())\n",
    "    headers = [\"Model\", \"Prompt\"] + metric_list\n",
    "    print_both(\"| \" + \" | \".join(headers) + \" |\")\n",
    "    print_both(\"| \" + \" | \".join(\"---\" for _ in headers) + \" |\")\n",
    "    for model_name, model_data in critique_data.items():\n",
    "        for prompt_name, prompt_data in model_data.items():\n",
    "            row = [model_name, prompt_name]\n",
    "            for metric_name, metric_data in prompt_data.items():\n",
    "                row.append(f\"{metric_data['overall']['value']:0.4f}\")\n",
    "            print_both(\"| \" + \" | \".join(row) + \" |\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Exploration\n",
    "\n",
    "You can also further explore the results on an example-by-example basis for a particular model, prompt, and metric. First, let's specify the model, prompt, and metric we want to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"cohere_medium\"\n",
    "prompt = \"standard\"\n",
    "metric = \"UniEval (Relevance)\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding High/low-performing Examples\n",
    "\n",
    "Here's an example of how you can sort the outputs and find particularly high-performing or low-performing examples to do error analysis. Here we're only outputting the system outputs and references because the summarization sources are long, but you could also print out the sources if you wanted to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_data = [x[\"value\"] for x in critique_data[model][prompt][metric][\"examples\"]]\n",
    "target_data = targets[model][prompt]\n",
    "graded_data = list(zip(input_data, target_data, metric_data))\n",
    "graded_data.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "def print_graded(input, target, metric_value):\n",
    "    print(f\"=== {metric_value:0.4f} ===\")\n",
    "    print(\"*** Reference ***\")\n",
    "    print(input[\"reference\"])\n",
    "    print()\n",
    "    print(\"*** Target ***\")\n",
    "    print(target)\n",
    "    print()\n",
    "\n",
    "print(f\"----- Best {metric} Examples for '{model} {prompt}' -----\")\n",
    "for input, target, metric_value in graded_data[:3]:\n",
    "    print_graded(input, target, metric_value)\n",
    "\n",
    "\n",
    "print(f\"----- Worst {metric} Examples for '{model} {prompt}' -----\")\n",
    "for input, target, metric_value in graded_data[:3]:\n",
    "    print_graded(input, target, metric_value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Examples Where One Method Outperforms Another\n",
    "\n",
    "When working with prompting, we'll often want to know which prompts are better than others and in which ways. First, let's specify two methods we're interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = \"cohere_medium\"\n",
    "prompt1 = \"standard\"\n",
    "model2 = \"cohere_medium\"\n",
    "prompt2 = \"tldr\"\n",
    "metric = \"UniEval (Relevance)\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can find examples where one method outperforms the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_data1 = [x[\"value\"] for x in critique_data[model1][prompt1][metric][\"examples\"]]\n",
    "target_data1 = targets[model1][prompt1]\n",
    "metric_data2 = [x[\"value\"] for x in critique_data[model2][prompt2][metric][\"examples\"]]\n",
    "target_data2 = targets[model2][prompt2]\n",
    "graded_data = list(zip(input_data, target_data1, target_data2, metric_data1, metric_data2))\n",
    "graded_data.sort(key=lambda x: x[3]-x[4], reverse=True)\n",
    "\n",
    "def print_graded(input, target1, target2, metric_value1, metric_value2):\n",
    "    print(f\"=== {metric_value1:0.4f} vs. {metric_value2:0.4f} ===\")\n",
    "    print(\"*** Reference ***\")\n",
    "    print(input[\"reference\"])\n",
    "    print()\n",
    "    print(\"*** Target 1 ***\")\n",
    "    print(target1)\n",
    "    print()\n",
    "    print(\"*** Target 2 ***\")\n",
    "    print(target2)\n",
    "    print()\n",
    "\n",
    "print(f\"----- Examples where '{model1} {prompt1}' outperforms '{model2} {prompt2}' on {metric} -----\")\n",
    "for input, target1, target2, metric_value1, metric_value2 in graded_data[:3]:\n",
    "    print_graded(input, target1, target2, metric_value1, metric_value2)\n",
    "\n",
    "print(f\"----- Examples where '{model2} {prompt2}' outperforms '{model1} {prompt1}' on {metric} -----\")\n",
    "for input, target1, target2, metric_value1, metric_value2 in graded_data[-3:]:\n",
    "    print_graded(input, target1, target2, metric_value1, metric_value2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Words\n",
    "\n",
    "That's it for today! Hope you had a good prompting workout, and if you have any comments, questions, or suggestions, drop us a line in [discord](https://discord.com/invite/vJHdpCBqWN) or through our [contact page](https://inspiredco.ai/contact/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5a491234ceba6570269415a7e5577eac39275bd65b6fc1709629ff5aba940f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
